Index: Analyzer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pymysql\r\nfrom gensim.corpora import Dictionary\r\nfrom gensim.models import LdaModel\r\nfrom gensim.parsing.preprocessing import preprocess_string\r\nfrom nltk.sentiment import SentimentIntensityAnalyzer\r\nimport nltk\r\nimport json\r\nfrom transformers import AutoTokenizer\r\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\r\n\r\nimport pandas as pd\r\nfrom transformers import pipeline\r\nfrom flair.models import TextClassifier\r\nfrom flair.data import Sentence\r\n\r\n# Download the vader_lexicon data\r\nnltk.download(\"vader_lexicon\")\r\n\r\n# Create a sentiment analyzer using NTLK's Vader\r\nanalyzer = SentimentIntensityAnalyzer()\r\n# Create a sentiment analyzer using distilbert-base-uncased-finetuned-sst-2-english\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\r\nbert_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\r\n\r\n# Load a pre-trained sentiment analysis model from Flair\r\nclassifier = TextClassifier.load('en-sentiment')\r\n\r\n\r\ndef classify_sentiment(polarity, polarity_threshold=0.05):\r\n    if polarity >= polarity_threshold:\r\n        return \"Positive\"\r\n    elif polarity <= -polarity_threshold:\r\n        return \"Negative\"\r\n    else:\r\n        return \"Neutral\"\r\n\r\n\r\ndef classify_sentiment_bert(review: str):\r\n    result = bert_pipeline(review)[0]\r\n    return result['label']\r\n\r\n\r\ndef classify_sentiment_ntlk(review: str):\r\n    scores = analyzer.polarity_scores(review)\r\n    return classify_sentiment(scores['compound'])\r\n\r\n\r\ndef classify_sentiment_flair(review):\r\n    sentence = Sentence(review)\r\n    classifier.predict(sentence)\r\n    return sentence.labels[0].value\r\n\r\n\r\ndef fetch_transcripts_from_db():\r\n    \"\"\"Fetch the transcripts from the chat_record table and return them.\"\"\"\r\n    connection = connect_to_db()\r\n    try:\r\n        with connection.cursor() as cursor:\r\n            sql = \"SELECT chat_record_id, transcript FROM chat_record\"\r\n            cursor.execute(sql)\r\n            return cursor.fetchall()\r\n    finally:\r\n        connection.close()\r\n\r\n\r\ndef process_transcript_for_review(transcript):\r\n    \"\"\"Process the chat transcript to extract messages from the Customer.\"\"\"\r\n    review = \"\"\r\n    try:\r\n        chat_entries = json.loads(transcript)\r\n        human_interaction_started = False\r\n        for entry in chat_entries:\r\n            # Check if a real human (not Talkative) is speaking\r\n            if any(key not in [\"Talkative\", \"Customer\"] for key in entry.keys()):\r\n                human_interaction_started = True\r\n            # If human interaction has started, process the Customer's messages\r\n            if human_interaction_started and \"Customer\" in entry:\r\n                review += entry[\"Customer\"] + \" \"\r\n    except Exception as e:\r\n        print(f\"Error processing transcript: {e}\")\r\n    return review.strip()\r\n\r\n\r\ndef get_reviews():\r\n    chat_records = fetch_transcripts_from_db()\r\n    reviews = []\r\n    for record in chat_records:\r\n        review = process_transcript_for_review(record['transcript'])\r\n        reviews.append({\r\n            \"chat_record_id\": record[\"chat_record_id\"],\r\n            \"review\": review\r\n        })\r\n    return reviews\r\n\r\n\r\ndef train_lda(reviews):\r\n    # Preprocess the reviews\r\n    texts = [preprocess_string(review) for review in reviews]\r\n\r\n    # Create a dictionary representation of the documents\r\n    dictionary = Dictionary(texts)\r\n\r\n    # Create a Bag of Words representation of the documents\r\n    corpus = [dictionary.doc2bow(text) for text in texts]\r\n\r\n    # Train the model on the corpus.\r\n    ldamodel = LdaModel(corpus, id2word=dictionary, num_topics=10)\r\n\r\n    return ldamodel, dictionary\r\n\r\n\r\ndef assign_topic_to_review(ldamodel, dictionary, review):\r\n    review = preprocess_string(review)\r\n    bow = dictionary.doc2bow(review)\r\n    topic_distribution = ldamodel.get_document_topics(bow)\r\n    main_topic_id, _ = max(topic_distribution, key=lambda x: x[1])\r\n    main_topic_words = ldamodel.show_topic(main_topic_id)\r\n    # Take the top 3 keywords\r\n    main_topic_readable = \", \".join([f\"{word} ({prob:.2f})\" for word, prob in main_topic_words[:3]])\r\n    return main_topic_readable\r\n\r\n\r\ndef get_sentiments_and_topics(reviews):\r\n    ldamodel, dictionary = train_lda(reviews)\r\n    results = []\r\n    for review in reviews:\r\n        result = {\r\n            \"Review\": review,\r\n            \"NTLK\": classify_sentiment_ntlk(review),\r\n            \"Flair\": classify_sentiment_flair(review),\r\n            \"BERT\": classify_sentiment_bert(review),  # 这里添加BERT的输出\r\n            \"Topic\": assign_topic_to_review(ldamodel, dictionary, review)\r\n        }\r\n        results.append(result)\r\n    return results\r\n\r\n\r\ndef save_results_to_json(results, filename):\r\n    with open(filename, 'w') as f:\r\n        json.dump(results, f, indent=4)\r\n\r\n\r\n# Database configuration\r\nDB_CONFIG = {\r\n    'host': '127.0.0.1',\r\n    'port': 3306,\r\n    'user': 'root',\r\n    'password': 'fty5005669',\r\n    'db': 'talkative',\r\n    'charset': 'utf8mb4',\r\n    'cursorclass': pymysql.cursors.DictCursor\r\n}\r\n\r\n\r\ndef connect_to_db():\r\n    \"\"\"Connect to the MySQL database and return the connection.\"\"\"\r\n    connection = pymysql.connect(**DB_CONFIG)\r\n    return connection\r\n\r\n\r\ndef ensure_chat_record_exists(chat_record_id):\r\n    connection = connect_to_db()\r\n    try:\r\n        with connection.cursor() as cursor:\r\n\r\n            sql = \"SELECT 1 FROM chat_record WHERE chat_record_id = %s\"\r\n            cursor.execute(sql, (chat_record_id,))\r\n            result = cursor.fetchone()\r\n            if not result:\r\n\r\n                sql = \"INSERT INTO chat_record (chat_record_id) VALUES (%s)\"\r\n                cursor.execute(sql, (chat_record_id,))\r\n                connection.commit()\r\n    finally:\r\n        connection.close()\r\n\r\n\r\ndef save_review_to_db(review_data):\r\n    chat_record_id = review_data.get('chat_record_id', None)\r\n    ensure_chat_record_exists(chat_record_id)\r\n    connection = connect_to_db()\r\n    try:\r\n        with connection.cursor() as cursor:\r\n            # Inserting data into review_analysis table\r\n            sql = \"\"\"\r\n                INSERT INTO review_analysis (\r\n                    review_text, sentiment_ntlk, sentiment_flair, sentiment_bert, topic, chat_record_id\r\n                )\r\n                VALUES (%s, %s, %s, %s, %s, %s)\r\n            \"\"\"\r\n            cursor.execute(sql, (\r\n                review_data['Review'],\r\n                review_data['NTLK'],\r\n                review_data['Flair'],\r\n                review_data['BERT'],\r\n                review_data['Topic'],\r\n                review_data.get('chat_record_id', None)\r\n            ))\r\n            connection.commit()\r\n    finally:\r\n        connection.close()\r\n\r\n\r\ndef main():\r\n    reviews_data = get_reviews()\r\n    results = get_sentiments_and_topics([data['review'] for data in reviews_data])\r\n\r\n    for idx, result in enumerate(results):\r\n        result['chat_record_id'] = reviews_data[idx]['chat_record_id']\r\n        save_review_to_db(result)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Analyzer.py b/Analyzer.py
--- a/Analyzer.py	(revision 10fbbd06980022eef7c75dfd3fb70ab466aad30c)
+++ b/Analyzer.py	(date 1691951193347)
@@ -146,7 +146,7 @@
     'host': '127.0.0.1',
     'port': 3306,
     'user': 'root',
-    'password': 'fty5005669',
+    'password': 'Fty5005669',
     'db': 'talkative',
     'charset': 'utf8mb4',
     'cursorclass': pymysql.cursors.DictCursor
